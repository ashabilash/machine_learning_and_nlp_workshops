{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb7685-30e1-457f-be20-4c57f7948723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "341ef275-57e1-4c5d-9138-0f5c0a6e2ea0",
   "metadata": {
    "id": "i3b6F2-nxtte"
   },
   "source": [
    "# Introducing Pandas Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4214f8-26a1-4264-bfd8-ac4395fb995e",
   "metadata": {
    "id": "qPTVP_xNxtte"
   },
   "source": [
    "At the very basic level, Pandas objects can be thought of as enhanced versions of NumPy structured arrays in which the rows and columns are identified with labels rather than simple integer indices.\n",
    "As we will see during the course of this chapter, Pandas provides a host of useful tools, methods, and functionality on top of the basic data structures, but nearly everything that follows will require an understanding of what these structures are.\n",
    "Thus, before we go any further, let's introduce these three fundamental Pandas data structures: the ``Series``, ``DataFrame``, and ``Index``.\n",
    "\n",
    "We will start our code sessions with the standard NumPy and Pandas imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6500e2af-a0ec-450b-91a7-11dcc526d7ba",
   "metadata": {
    "id": "pyrfBOkVxttg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58371f40-8328-48f2-9b7b-c428ad78ec13",
   "metadata": {
    "id": "DFY0E7kyxtti"
   },
   "source": [
    "## The Pandas Series Object\n",
    "\n",
    "A Pandas ``Series`` is a one-dimensional array of indexed data.\n",
    "It can be created from a list or array as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4943ca9d-0cf9-489b-8f0c-0c4767a0977c",
   "metadata": {
    "id": "O4TjeOzfxtti",
    "outputId": "f888a00e-45b2-454c-bb56-b2a7187703d1"
   },
   "outputs": [],
   "source": [
    "data = pd.Series([0.25, 0.5, 0.75, 1.0])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3da36-2b6b-429f-9549-74dc3c300083",
   "metadata": {
    "id": "exIOD0lBxttl"
   },
   "source": [
    "As we see in the output, the ``Series`` wraps both a sequence of values and a sequence of indices, which we can access with the ``values`` and ``index`` attributes.\n",
    "The ``values`` are simply a familiar NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c700982-3ba1-490e-b724-7e3ec3b24a14",
   "metadata": {
    "id": "WZ3UU2bvxttm",
    "outputId": "005a4b86-4ad5-4593-b8b9-da266311dad2"
   },
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4af5dc3-29c3-4650-b0f2-47f3bb15cb5c",
   "metadata": {
    "id": "uEppiMjqxttn"
   },
   "source": [
    "The ``index`` is an array-like object of type ``pd.Index``, which we'll discuss in more detail momentarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a7a99-2bdc-4339-be74-e731117e44d1",
   "metadata": {
    "id": "RKptnc7jxtto",
    "outputId": "1cf08486-aa4c-450f-e812-29bbad583209"
   },
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5fd7b-19b7-4113-a724-0986b0da97c5",
   "metadata": {
    "id": "aFaCl1qdxtto"
   },
   "source": [
    "Like with a NumPy array, data can be accessed by the associated index via the familiar Python square-bracket notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4291bd-c229-4e24-9e8a-793dfb92c1e7",
   "metadata": {
    "id": "cMIZz0EKxttp",
    "outputId": "12263956-25ae-4a31-89e9-ffd1382decff"
   },
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52775f04-1949-48c3-a415-63f82f13394d",
   "metadata": {
    "id": "spUywJj6xttp",
    "outputId": "3b769dfb-9a73-4325-dfeb-fcf117f44fc4"
   },
   "outputs": [],
   "source": [
    "data[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889a929-a916-4563-b37a-34802162c912",
   "metadata": {
    "id": "DNp_DcL9xttq"
   },
   "source": [
    "As we will see, though, the Pandas ``Series`` is much more general and flexible than the one-dimensional NumPy array that it emulates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab9e52-990a-4e94-b88a-5fc69ffcd7b0",
   "metadata": {
    "id": "0k-IC3mfxttq"
   },
   "source": [
    "### ``Series`` as generalized NumPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf58d708-d9ec-4af5-8174-68dcca93e97a",
   "metadata": {
    "id": "xSRMe533xttr"
   },
   "source": [
    "From what we've seen so far, it may look like the ``Series`` object is basically interchangeable with a one-dimensional NumPy array.\n",
    "The essential difference is the presence of the index: while the Numpy Array has an *implicitly defined* integer index used to access the values, the Pandas ``Series`` has an *explicitly defined* index associated with the values.\n",
    "\n",
    "This explicit index definition gives the ``Series`` object additional capabilities. For example, the index need not be an integer, but can consist of values of any desired type.\n",
    "For example, if we wish, we can use strings as an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c04bb20-db36-4251-8573-c28fa6357e8f",
   "metadata": {
    "id": "grjwQtdQxtts",
    "outputId": "ffbf7fa6-ef3c-4c94-c0e1-50fe8c256e10"
   },
   "outputs": [],
   "source": [
    "data = pd.Series([0.25, 0.5, 0.75, 1.0],\n",
    "                 index=['a', 'b', 'c', 'd'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa038fc1-491c-4142-b02b-bef556bd06d9",
   "metadata": {
    "id": "iVPvI98Dxtts"
   },
   "source": [
    "And the item access works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fc68ca-2013-4ad9-b6da-687e8ef88f39",
   "metadata": {
    "id": "RrDSU5RYxttt",
    "outputId": "f8daddc4-e6a8-4f5d-c12f-28f9e641488a"
   },
   "outputs": [],
   "source": [
    "data['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e2c77-1aa6-44e5-ae9a-e63bfd9d35db",
   "metadata": {
    "id": "L2xrZZcLxttu"
   },
   "source": [
    "We can even use non-contiguous or non-sequential indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857064f-50d7-475d-99d4-660846565a47",
   "metadata": {
    "id": "3Npw4XQvxttu",
    "outputId": "6835dace-9993-4f61-c915-09de87da599a"
   },
   "outputs": [],
   "source": [
    "data = pd.Series([0.25, 0.5, 0.75, 1.0],\n",
    "                 index=[2, 5, 3, 7])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa02092-f9f5-40f3-b7c2-1e039dafed15",
   "metadata": {
    "id": "RMs6LRXMxttu",
    "outputId": "5c310a7d-8a7e-497f-e027-5447d45372a4"
   },
   "outputs": [],
   "source": [
    "data[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f8bcf-bce7-46b5-a707-47780483adb0",
   "metadata": {
    "id": "0ZyDQ4UUxttv"
   },
   "source": [
    "### Series as specialized dictionary\n",
    "\n",
    "In this way, you can think of a Pandas ``Series`` a bit like a specialization of a Python dictionary.\n",
    "A dictionary is a structure that maps arbitrary keys to a set of arbitrary values, and a ``Series`` is a structure which maps typed keys to a set of typed values.\n",
    "This typing is important: just as the type-specific compiled code behind a NumPy array makes it more efficient than a Python list for certain operations, the type information of a Pandas ``Series`` makes it much more efficient than Python dictionaries for certain operations.\n",
    "\n",
    "The ``Series``-as-dictionary analogy can be made even more clear by constructing a ``Series`` object directly from a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e84167d-8e54-4d47-a999-11cc7c5e78a2",
   "metadata": {
    "id": "ltPKhxOTxttv",
    "outputId": "5ca90c3f-5be7-4905-acd5-9e48c40569fa"
   },
   "outputs": [],
   "source": [
    "population_dict = {'California': 38332521,\n",
    "                   'Texas': 26448193,\n",
    "                   'New York': 19651127,\n",
    "                   'Florida': 19552860,\n",
    "                   'Illinois': 12882135}\n",
    "population = pd.Series(population_dict)\n",
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ac4ea-3cfe-48bc-b703-cb603ef85747",
   "metadata": {
    "id": "n8VdTOTrxttw"
   },
   "source": [
    "By default, a ``Series`` will be created where the index is drawn from the sorted keys.\n",
    "From here, typical dictionary-style item access can be performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7e0693-37dc-41de-9f78-c058b73ae918",
   "metadata": {
    "id": "mVy5sO0Jxttw",
    "outputId": "a974f6ad-1ecb-4d7d-8b1e-edada68d6231"
   },
   "outputs": [],
   "source": [
    "population['California']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac66ec-4677-4da0-b71c-13b4235b35e4",
   "metadata": {
    "id": "Ce-Ogh3Rxttw"
   },
   "source": [
    "Unlike a dictionary, though, the ``Series`` also supports array-style operations such as slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0f362-15ba-4bb0-a5ed-51821ecae2e4",
   "metadata": {
    "id": "oIVtjO_Gxttx",
    "outputId": "af19529d-98e6-42ef-d0dd-313e750d18c9"
   },
   "outputs": [],
   "source": [
    "population['California':'Illinois']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d935119-016c-43cd-94b6-5baa054f25d4",
   "metadata": {
    "id": "Hgpf7Ogtxttx"
   },
   "source": [
    "We'll discuss some of the quirks of Pandas indexing and slicing in [Data Indexing and Selection](03.02-Data-Indexing-and-Selection.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b25541-cb13-433f-9114-e6badd0d6908",
   "metadata": {
    "id": "26kiFBhnxttx"
   },
   "source": [
    "### Constructing Series objects\n",
    "\n",
    "We've already seen a few ways of constructing a Pandas ``Series`` from scratch; all of them are some version of the following:\n",
    "\n",
    "```python\n",
    ">>> pd.Series(data, index=index)\n",
    "```\n",
    "\n",
    "where ``index`` is an optional argument, and ``data`` can be one of many entities.\n",
    "\n",
    "For example, ``data`` can be a list or NumPy array, in which case ``index`` defaults to an integer sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1b1bd-1057-42ba-821a-78272b5c3b79",
   "metadata": {
    "id": "5mtRinfextty",
    "outputId": "633e10e7-333b-4afe-f3b7-3f4b7f32539b"
   },
   "outputs": [],
   "source": [
    "pd.Series([2, 4, 6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8b149-cf26-4f0d-9799-2b257b8ec1b9",
   "metadata": {
    "id": "XKEp-bzextty"
   },
   "source": [
    "``data`` can be a scalar, which is repeated to fill the specified index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc6ad52-04ee-47ce-9eef-c2c38f222cfd",
   "metadata": {
    "id": "YQpOGFExxtty",
    "outputId": "d1bbd6a4-0d9f-4696-f8ab-6eb06bab5bb4"
   },
   "outputs": [],
   "source": [
    "pd.Series(5, index=[100, 200, 300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5fbefe-c63f-487b-9c5f-5e9b04983fd1",
   "metadata": {
    "id": "UCcdtLPGxtty"
   },
   "source": [
    "``data`` can be a dictionary, in which ``index`` defaults to the sorted dictionary keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e9b6e9-237d-49c7-9090-19c0ca7fdd8c",
   "metadata": {
    "id": "xNQghkodxtty",
    "outputId": "47f404de-139c-4593-aa85-17636a90c42f"
   },
   "outputs": [],
   "source": [
    "pd.Series({2:'a', 1:'b', 3:'c'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e357d259-f5fb-4ce4-837e-27010e56a36e",
   "metadata": {
    "id": "8i6iFZ86xttz"
   },
   "source": [
    "In each case, the index can be explicitly set if a different result is preferred:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427bb918-b097-4d8b-ba5a-2d0fcfe6fa44",
   "metadata": {
    "id": "ayJlDhVXxttz",
    "outputId": "3822fe6f-5703-4ac2-a662-68aded0cd6b0"
   },
   "outputs": [],
   "source": [
    "pd.Series({2:'a', 1:'b', 3:'c'}, index=[3, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17a511-a086-4b03-b834-489b783a3307",
   "metadata": {
    "id": "HLiqGg-8xttz"
   },
   "source": [
    "Notice that in this case, the ``Series`` is populated only with the explicitly identified keys."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f996e73-78af-4a5e-a05b-bf345db39d0a",
   "metadata": {
    "id": "kTv2Xce_xtt0"
   },
   "source": [
    "## The Pandas DataFrame Object\n",
    "\n",
    "The next fundamental structure in Pandas is the ``DataFrame``.\n",
    "Like the ``Series`` object discussed in the previous section, the ``DataFrame`` can be thought of either as a generalization of a NumPy array, or as a specialization of a Python dictionary.\n",
    "We'll now take a look at each of these perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790aedd1-1a21-4076-92eb-2a54cff30c5c",
   "metadata": {
    "id": "OQBH8Mfkxtt0"
   },
   "source": [
    "### DataFrame as a generalized NumPy array\n",
    "If a ``Series`` is an analog of a one-dimensional array with flexible indices, a ``DataFrame`` is an analog of a two-dimensional array with both flexible row indices and flexible column names.\n",
    "Just as you might think of a two-dimensional array as an ordered sequence of aligned one-dimensional columns, you can think of a ``DataFrame`` as a sequence of aligned ``Series`` objects.\n",
    "Here, by \"aligned\" we mean that they share the same index.\n",
    "\n",
    "To demonstrate this, let's first construct a new ``Series`` listing the area of each of the five states discussed in the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7de2a2-cf8f-4998-988c-26e8f8976066",
   "metadata": {
    "id": "T9SRcH3lxtt0",
    "outputId": "fc353335-7dfb-482d-a730-7e8c09aef2d7"
   },
   "outputs": [],
   "source": [
    "area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,\n",
    "             'Florida': 170312, 'Illinois': 149995}\n",
    "area = pd.Series(area_dict)\n",
    "area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f3fe1-76c8-4f2a-b0fc-a94879a0f0cf",
   "metadata": {
    "id": "nALnG2lvxtt0"
   },
   "source": [
    "Now that we have this along with the ``population`` Series from before, we can use a dictionary to construct a single two-dimensional object containing this information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45451bb8-3a86-486d-956c-c049cf6f64c3",
   "metadata": {
    "id": "YrQhCprfxtt1",
    "outputId": "058e29e8-4d88-4c56-d3ee-f20d52759d74"
   },
   "outputs": [],
   "source": [
    "states = pd.DataFrame({'population': population,\n",
    "                       'area': area})\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9367f8d1-26cb-4751-b6c4-0a48894c90db",
   "metadata": {
    "id": "aOTL1QT8xtt1"
   },
   "source": [
    "Like the ``Series`` object, the ``DataFrame`` has an ``index`` attribute that gives access to the index labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181f838-0b94-4f5d-96c4-25f384b9b912",
   "metadata": {
    "id": "Zb1Fnkijxtt1",
    "outputId": "fa944910-3576-4fa3-e7f5-a527089f53ee"
   },
   "outputs": [],
   "source": [
    "states.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa92079a-b85b-4193-a16f-7f2375d33962",
   "metadata": {
    "id": "t6VOp0aUxtt1"
   },
   "source": [
    "Additionally, the ``DataFrame`` has a ``columns`` attribute, which is an ``Index`` object holding the column labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c3c518-4e16-4598-bd61-2b85e6d69f79",
   "metadata": {
    "id": "dd1wOPXVxtt2",
    "outputId": "19289039-2f49-45e6-b650-7ca63c029609"
   },
   "outputs": [],
   "source": [
    "states.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584a08f3-232c-4e34-ac46-baf0224b6c2a",
   "metadata": {
    "id": "IGrNrXk7xtt2"
   },
   "source": [
    "Thus the ``DataFrame`` can be thought of as a generalization of a two-dimensional NumPy array, where both the rows and columns have a generalized index for accessing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810b254b-2f28-4c9f-812b-fe2cb2b8474d",
   "metadata": {
    "id": "BzuN-QuFxtt2"
   },
   "source": [
    "### DataFrame as specialized dictionary\n",
    "\n",
    "Similarly, we can also think of a ``DataFrame`` as a specialization of a dictionary.\n",
    "Where a dictionary maps a key to a value, a ``DataFrame`` maps a column name to a ``Series`` of column data.\n",
    "For example, asking for the ``'area'`` attribute returns the ``Series`` object containing the areas we saw earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dab265-cbf3-446a-be01-429d2d0951c5",
   "metadata": {
    "id": "TlxR6-kVxtt2",
    "outputId": "10913ff7-b140-4953-8875-c0a5a2bbcb76"
   },
   "outputs": [],
   "source": [
    "states['area']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f830705-46b3-4325-bb74-f6c459b63dcf",
   "metadata": {
    "id": "ZDfgh8Sqxtt2"
   },
   "source": [
    "Notice the potential point of confusion here: in a two-dimesnional NumPy array, ``data[0]`` will return the first *row*. For a ``DataFrame``, ``data['col0']`` will return the first *column*.\n",
    "Because of this, it is probably better to think about ``DataFrame``s as generalized dictionaries rather than generalized arrays, though both ways of looking at the situation can be useful.\n",
    "We'll explore more flexible means of indexing ``DataFrame``s in [Data Indexing and Selection](03.02-Data-Indexing-and-Selection.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14464787-b21e-410b-af37-f11a1a70e2fd",
   "metadata": {
    "id": "qRKx0vpPxtt3"
   },
   "source": [
    "### Constructing DataFrame objects\n",
    "\n",
    "A Pandas ``DataFrame`` can be constructed in a variety of ways.\n",
    "Here we'll give several examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf8c5b-ec06-4300-9933-90c3425b4e89",
   "metadata": {
    "id": "m019QSBOxtt3"
   },
   "source": [
    "#### From a single Series object\n",
    "\n",
    "A ``DataFrame`` is a collection of ``Series`` objects, and a single-column ``DataFrame`` can be constructed from a single ``Series``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e23b88-39b8-4f92-9dfc-db088d50945a",
   "metadata": {
    "id": "A7bMD8COxtt3",
    "outputId": "82256283-402f-4058-dc37-bee16ad3a11f"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(population, columns=['population'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73890d3c-b445-4dd2-b069-644a586dd40b",
   "metadata": {
    "id": "OWJZ6UKaxtt3"
   },
   "source": [
    "#### From a list of dicts\n",
    "\n",
    "Any list of dictionaries can be made into a ``DataFrame``.\n",
    "We'll use a simple list comprehension to create some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a77d5-81e1-4c61-98f0-35acf2e7a9bd",
   "metadata": {
    "id": "PfUGT8z1xtt3",
    "outputId": "022ca6e2-1534-494d-8bd2-6985f6f0e1a2"
   },
   "outputs": [],
   "source": [
    "data = [{'a': i, 'b': 2 * i}\n",
    "        for i in range(3)]\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43af2137-a582-4f46-8139-0298dc7b1b1f",
   "metadata": {
    "id": "KaD0M4N1xtt4"
   },
   "source": [
    "Even if some keys in the dictionary are missing, Pandas will fill them in with ``NaN`` (i.e., \"not a number\") values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a724e9a4-b271-4a0c-a5f6-9fb586ca7f40",
   "metadata": {
    "id": "IJRd0n7Hxtt4",
    "outputId": "f58e8db8-1823-4749-e4c0-50ca95b660fb"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([{'a': 1, 'b': 2}, {'b': 3, 'c': 4}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab2e46-174c-44c0-8f56-580f0acad12c",
   "metadata": {
    "id": "ijCtdwoCxtt4"
   },
   "source": [
    "#### From a dictionary of Series objects\n",
    "\n",
    "As we saw before, a ``DataFrame`` can be constructed from a dictionary of ``Series`` objects as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26854466-6651-4f94-887e-3d790f1bc57e",
   "metadata": {
    "id": "K6DKhVdoxtt4",
    "outputId": "710dbd95-1bfe-4546-eac0-acf99f736845"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({'population': population,\n",
    "              'area': area})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de07192a-036e-4298-8afd-33e68d3bf82a",
   "metadata": {
    "id": "DvCvPqWfxtt5"
   },
   "source": [
    "#### From a two-dimensional NumPy array\n",
    "\n",
    "Given a two-dimensional array of data, we can create a ``DataFrame`` with any specified column and index names.\n",
    "If omitted, an integer index will be used for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9ce45c-1fc7-4a6d-9f79-ff7a52804448",
   "metadata": {
    "id": "0Y3tAwYixtt5",
    "outputId": "51b9d1d7-9ea3-4c2b-afdb-8bc866b32504"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(np.random.rand(3, 2),\n",
    "             columns=['foo', 'bar'],\n",
    "             index=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260825f3-f14e-43fe-be7c-6372604bde74",
   "metadata": {
    "id": "898Jvf_txtt5"
   },
   "source": [
    "#### From a NumPy structured array\n",
    "\n",
    "We covered structured arrays in [Structured Data: NumPy's Structured Arrays](02.09-Structured-Data-NumPy.ipynb).\n",
    "A Pandas ``DataFrame`` operates much like a structured array, and can be created directly from one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99627df6-0e41-4011-b7f1-56f5e9c7e6f3",
   "metadata": {
    "id": "CUBXoYyvxtt5",
    "outputId": "a8d37d32-1ec0-445a-fd17-cb74b03b3922"
   },
   "outputs": [],
   "source": [
    "A = np.zeros(3, dtype=[('A', 'i8'), ('B', 'f8')])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0dfe95-299a-4611-81e9-6b3aeecca7f8",
   "metadata": {
    "id": "Ww1y40gsxtt6",
    "outputId": "0829b332-3ac9-4c21-81e3-2aa574202ff7"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cede23-b8a2-4b31-b21c-632a09185c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b12eb7f2-7d87-4688-ae40-7bcdda41d7ba",
   "metadata": {
    "id": "IbHWoKhQxx2z"
   },
   "source": [
    "# Combining Datasets: Concat and Append"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03fea9-7f89-4d3f-939d-c6c6c33e4fec",
   "metadata": {
    "id": "pvQx6nMaxx20"
   },
   "source": [
    "Some of the most interesting studies of data come from combining different data sources.\n",
    "These operations can involve anything from very straightforward concatenation of two different datasets, to more complicated database-style joins and merges that correctly handle any overlaps between the datasets.\n",
    "``Series`` and ``DataFrame``s are built with this type of operation in mind, and Pandas includes functions and methods that make this sort of data wrangling fast and straightforward.\n",
    "\n",
    "Here we'll take a look at simple concatenation of ``Series`` and ``DataFrame``s with the ``pd.concat`` function; later we'll dive into more sophisticated in-memory merges and joins implemented in Pandas.\n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2adf18-8bde-45f8-b9e8-8eed12ea968b",
   "metadata": {
    "id": "X8miJ-OBxx22",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727024df-4a2c-4518-b7d2-a4c90491836d",
   "metadata": {
    "id": "lLudmguVxx25"
   },
   "source": [
    "For convenience, we'll define this function which creates a ``DataFrame`` of a particular form that will be useful below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b961c6c-960a-41d8-861f-325cccb37645",
   "metadata": {
    "id": "LedQwhVtxx26",
    "outputId": "a42214f7-b214-44d0-9edf-d2c5224959c8"
   },
   "outputs": [],
   "source": [
    "def make_df(cols, ind):\n",
    "    \"\"\"Quickly make a DataFrame\"\"\"\n",
    "    data = {c: [str(c) + str(i) for i in ind]\n",
    "            for c in cols}\n",
    "    return pd.DataFrame(data, ind)\n",
    "\n",
    "# example DataFrame\n",
    "make_df('ABC', range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a0bd9-08af-4777-8098-aadee34ba353",
   "metadata": {
    "id": "aS-BB2ZHxx2-"
   },
   "source": [
    "In addition, we'll create a quick class that allows us to display multiple ``DataFrame``s side by side. The code makes use of the special ``_repr_html_`` method, which IPython uses to implement its rich object display:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d070de-6c30-4e0d-8f75-a0a98a8c4c79",
   "metadata": {
    "id": "fsTVBp4dxx3A",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
    "                         for a in self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
    "                           for a in self.args)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee80971-4e30-426d-abb1-97a7bf9b110f",
   "metadata": {
    "id": "041jE9Fcxx3C"
   },
   "source": [
    "The use of this will become clearer as we continue our discussion in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1241b5c7-13fb-45e2-9a11-483f2bd7a29c",
   "metadata": {
    "id": "SlFAvNhLxx3E"
   },
   "source": [
    "## Recall: Concatenation of NumPy Arrays\n",
    "\n",
    "Concatenation of ``Series`` and ``DataFrame`` objects is very similar to concatenation of Numpy arrays, which can be done via the ``np.concatenate`` function as discussed in [The Basics of NumPy Arrays](02.02-The-Basics-Of-NumPy-Arrays.ipynb).\n",
    "Recall that with it, you can combine the contents of two or more arrays into a single array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998700e-4528-483e-9d2c-469a3dd7d21a",
   "metadata": {
    "id": "Ntglhi6Txx3F",
    "outputId": "2e311094-dcf1-457f-fa9e-2c65a04e813b"
   },
   "outputs": [],
   "source": [
    "x = [1, 2, 3]\n",
    "y = [4, 5, 6]\n",
    "z = [7, 8, 9]\n",
    "np.concatenate([x, y, z])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4f8f53-0144-4331-8a52-8f75dfa17e72",
   "metadata": {
    "id": "_HeJIOb4xx3G"
   },
   "source": [
    "The first argument is a list or tuple of arrays to concatenate.\n",
    "Additionally, it takes an ``axis`` keyword that allows you to specify the axis along which the result will be concatenated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7027f23-a199-4f5d-aef1-f3d8a74d892a",
   "metadata": {
    "id": "i1Q67iIJxx3H",
    "outputId": "7e1ba049-c526-40fb-e82d-b129c8d2f81f"
   },
   "outputs": [],
   "source": [
    "x = [[1, 2],\n",
    "     [3, 4]]\n",
    "np.concatenate([x, x], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e562d-4a22-4a7b-82e7-b05164a1c757",
   "metadata": {
    "id": "aSjanvw3xx3H"
   },
   "source": [
    "## Simple Concatenation with ``pd.concat``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65576b0-4343-42b7-83a6-79f48897a327",
   "metadata": {
    "id": "Z6LSSh2pxx3I"
   },
   "source": [
    "Pandas has a function, ``pd.concat()``, which has a similar syntax to ``np.concatenate`` but contains a number of options that we'll discuss momentarily:\n",
    "\n",
    "```python\n",
    "# Signature in Pandas v0.18\n",
    "pd.concat(objs, axis=0, join='outer', join_axes=None, ignore_index=False,\n",
    "          keys=None, levels=None, names=None, verify_integrity=False,\n",
    "          copy=True)\n",
    "```\n",
    "\n",
    "``pd.concat()`` can be used for a simple concatenation of ``Series`` or ``DataFrame`` objects, just as ``np.concatenate()`` can be used for simple concatenations of arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3c9124-8123-498d-b8db-3ea58de4779e",
   "metadata": {
    "id": "v1cOdnF0xx3K",
    "outputId": "a3b491fd-e25b-4abe-86c4-5afe70b85925"
   },
   "outputs": [],
   "source": [
    "ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])\n",
    "ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])\n",
    "pd.concat([ser1, ser2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc76516f-46b4-44fe-b3f8-03b6d56a11b5",
   "metadata": {
    "id": "u9DaU89Sxx3L"
   },
   "source": [
    "It also works to concatenate higher-dimensional objects, such as ``DataFrame``s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd321093-6f44-4b81-a042-6a8d1767193e",
   "metadata": {
    "id": "MDWcqRc1xx3L",
    "outputId": "7ed2b8f0-2644-4324-9cde-ada7f94b5f39"
   },
   "outputs": [],
   "source": [
    "df1 = make_df('AB', [1, 2])\n",
    "df2 = make_df('AB', [3, 4])\n",
    "display('df1', 'df2', 'pd.concat([df1, df2])')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce36f6f4-2acf-4b52-8d8e-8661c318b9ae",
   "metadata": {
    "id": "s_K7ESpexx3M"
   },
   "source": [
    "By default, the concatenation takes place row-wise within the ``DataFrame`` (i.e., ``axis=0``).\n",
    "Like ``np.concatenate``, ``pd.concat`` allows specification of an axis along which concatenation will take place.\n",
    "Consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398f6d0-a3c1-467f-8cdc-7b386f26cd17",
   "metadata": {
    "id": "3bqvewcAxx3O",
    "outputId": "be24c9a6-e445-4e6d-b5d3-21e300506520"
   },
   "outputs": [],
   "source": [
    "df3 = make_df('AB', [0, 1])\n",
    "df4 = make_df('CD', [0, 1])\n",
    "display('df3', 'df4', \"pd.concat([df3, df4], axis=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbfa803-b771-4cad-bae7-e45f383f4290",
   "metadata": {
    "id": "BGlbNyxgxx3x"
   },
   "source": [
    "We could have equivalently specified ``axis=1``; here we've used the more intuitive ``axis='col'``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c85f96-27d6-4749-9d80-e14b5dc5a4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cfb09ee-0a07-40e7-9aa2-ff00237120ff",
   "metadata": {
    "id": "N1AgKjZFxx4F"
   },
   "source": [
    "### Concatenation with joins\n",
    "\n",
    "In the simple examples we just looked at, we were mainly concatenating ``DataFrame``s with shared column names.\n",
    "In practice, data from different sources might have different sets of column names, and ``pd.concat`` offers several options in this case.\n",
    "Consider the concatenation of the following two ``DataFrame``s, which have some (but not all!) columns in common:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2682065e-bee2-41de-b19b-c8044a734131",
   "metadata": {
    "id": "q24gJSTxxx4G",
    "outputId": "2950e4ed-8669-4282-c4d9-5d4e688dfd14"
   },
   "outputs": [],
   "source": [
    "df5 = make_df('ABC', [1, 2])\n",
    "df6 = make_df('BCD', [3, 4])\n",
    "display('df5', 'df6', 'pd.concat([df5, df6])')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a35b1b-f593-4e8b-a45e-b4de389526ec",
   "metadata": {
    "id": "J1MLcnVdxx4H"
   },
   "source": [
    "By default, the entries for which no data is available are filled with NA values.\n",
    "To change this, we can specify one of several options for the ``join`` and ``join_axes`` parameters of the concatenate function.\n",
    "By default, the join is a union of the input columns (``join='outer'``), but we can change this to an intersection of the columns using ``join='inner'``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120c04b9-7b1a-4f9a-a965-c4523573d669",
   "metadata": {
    "id": "y1njjOtLxx4J",
    "outputId": "701267a1-d364-4ada-ce86-2c97c3c87480"
   },
   "outputs": [],
   "source": [
    "display('df5', 'df6',\n",
    "        \"pd.concat([df5, df6], join='inner')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73803aeb-dec1-459d-89fd-f5570d0b9189",
   "metadata": {
    "id": "Ay2RAhXHxx4K"
   },
   "source": [
    "Another option is to directly specify the index of the remaininig colums using the ``join_axes`` argument, which takes a list of index objects.\n",
    "Here we'll specify that the returned columns should be the same as those of the first input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff028466-0c40-4a93-976b-7771fb586786",
   "metadata": {
    "id": "tzV-eRVpxx4N",
    "outputId": "fddc8905-6c64-49b7-f9d8-fd64328d54ce"
   },
   "outputs": [],
   "source": [
    "display('df5', 'df6',\n",
    "        \"pd.concat([df5, df6], join_axes=[df5.columns])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee48d71-6202-4260-a874-9a689be5cb24",
   "metadata": {
    "id": "J8P0QWjpxx4O"
   },
   "source": [
    "The combination of options of the ``pd.concat`` function allows a wide range of possible behaviors when joining two datasets; keep these in mind as you use these tools for your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc1b25a-e9d8-4962-bbaf-6e870acef289",
   "metadata": {
    "id": "5elC03A2xx4O"
   },
   "source": [
    "### The ``append()`` method\n",
    "\n",
    "Because direct array concatenation is so common, ``Series`` and ``DataFrame`` objects have an ``append`` method that can accomplish the same thing in fewer keystrokes.\n",
    "For example, rather than calling ``pd.concat([df1, df2])``, you can simply call ``df1.append(df2)``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2d86a-ac7b-4516-a050-6a2d7cddb9dc",
   "metadata": {
    "id": "OYromIIFxx4P",
    "outputId": "5f438e93-c13a-4e6b-cf7b-1862adaaf0cf"
   },
   "outputs": [],
   "source": [
    "display('df1', 'df2', 'df1.append(df2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11e8b8b-e577-4ed5-9ab2-bbb17155185d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345ed433-9163-4d38-9b9c-2324872542a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b15d6a9-9d9c-4608-97a1-bad5d870cae9",
   "metadata": {
    "id": "jbJeiWwvxymm"
   },
   "source": [
    "# Combining Datasets: Merge and Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0828590f-0864-4f5c-8947-846f92f8c928",
   "metadata": {
    "id": "yN4O3WlAxyms"
   },
   "source": [
    "One essential feature offered by Pandas is its high-performance, in-memory join and merge operations.\n",
    "If you have ever worked with databases, you should be familiar with this type of data interaction.\n",
    "The main interface for this is the ``pd.merge`` function, and we'll see few examples of how this can work in practice.\n",
    "\n",
    "For convenience, we will start by redefining the ``display()`` functionality from the previous section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9056c24d-93a7-469b-940c-9fe77f2db47a",
   "metadata": {
    "id": "GA4sOsM8xymv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class display(object):\n",
    "    \"\"\"Display HTML representation of multiple objects\"\"\"\n",
    "    template = \"\"\"<div style=\"float: left; padding: 10px;\">\n",
    "    <p style='font-family:\"Courier New\", Courier, monospace'>{0}</p>{1}\n",
    "    </div>\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        self.args = args\n",
    "        \n",
    "    def _repr_html_(self):\n",
    "        return '\\n'.join(self.template.format(a, eval(a)._repr_html_())\n",
    "                         for a in self.args)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n'.join(a + '\\n' + repr(eval(a))\n",
    "                           for a in self.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46e3ab4-433f-49e2-8316-05e9609b9703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9379261d-c832-4a37-8448-5edff71eda15",
   "metadata": {
    "id": "BoLnRqMYxym6"
   },
   "source": [
    "### One-to-one joins\n",
    "\n",
    "Perhaps the simplest type of merge expresion is the one-to-one join, which is in many ways very similar to the column-wise concatenation seen in [Combining Datasets: Concat & Append](03.06-Concat-And-Append.ipynb).\n",
    "As a concrete example, consider the following two ``DataFrames`` which contain information on several employees in a company:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ba26d-b9ce-4406-8016-6748ea2b4bd6",
   "metadata": {
    "id": "0wkPAsLOxym9",
    "outputId": "f4b6f982-30b5-433e-b03c-e83ea19af6dc"
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})\n",
    "display('df1', 'df2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f01c1-3cb1-4339-aae3-6eabeb56d95f",
   "metadata": {
    "id": "sdnkuf36xynC"
   },
   "source": [
    "To combine this information into a single ``DataFrame``, we can use the ``pd.merge()`` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f046d5a-745b-4adc-8023-a4f3cdd4c406",
   "metadata": {
    "id": "WfI24UjOxynD",
    "outputId": "53977426-03e3-4358-a0c2-5040486e6957"
   },
   "outputs": [],
   "source": [
    "df3 = pd.merge(df1, df2)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cf14cb-1b1d-4188-8a62-5c134189a144",
   "metadata": {
    "id": "qaX-jVz0xynF"
   },
   "source": [
    "The ``pd.merge()`` function recognizes that each ``DataFrame`` has an \"employee\" column, and automatically joins using this column as a key.\n",
    "The result of the merge is a new ``DataFrame`` that combines the information from the two inputs.\n",
    "Notice that the order of entries in each column is not necessarily maintained: in this case, the order of the \"employee\" column differs between ``df1`` and ``df2``, and the ``pd.merge()`` function correctly accounts for this.\n",
    "Additionally, keep in mind that the merge in general discards the index, except in the special case of merges by index (see the ``left_index`` and ``right_index`` keywords, discussed momentarily)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56b0e6-ea4b-4a8b-b887-2970bd781b80",
   "metadata": {
    "id": "j4hp750-xynG"
   },
   "source": [
    "### Many-to-one joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9da593-8613-4c21-b0f0-8588652040c8",
   "metadata": {
    "id": "TYJ-f06ZxynH"
   },
   "source": [
    "Many-to-one joins are joins in which one of the two key columns contains duplicate entries.\n",
    "For the many-to-one case, the resulting ``DataFrame`` will preserve those duplicate entries as appropriate.\n",
    "Consider the following example of a many-to-one join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e16fbc0-215a-44f0-ba83-3b3060482d6c",
   "metadata": {
    "id": "svj1o02zxynK",
    "outputId": "c3f93f41-15a1-4332-ad78-656dfb84e6d4"
   },
   "outputs": [],
   "source": [
    "df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\n",
    "                    'supervisor': ['Carly', 'Guido', 'Steve']})\n",
    "display('df3', 'df4', 'pd.merge(df3, df4)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c78b8a-bb9e-4e8a-b554-fcb937bb6bb7",
   "metadata": {
    "id": "jNwYh7ONxynM"
   },
   "source": [
    "The resulting ``DataFrame`` has an aditional column with the \"supervisor\" information, where the information is repeated in one or more locations as required by the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566661d-c1c2-4bf8-be18-6ee5c9cd1f99",
   "metadata": {
    "id": "BtYR0la_xynN"
   },
   "source": [
    "### Many-to-many joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd06aa-ffc8-4ae9-b717-aeb7f6da9881",
   "metadata": {
    "id": "pFX6Yt0CxynO"
   },
   "source": [
    "Many-to-many joins are a bit confusing conceptually, but are nevertheless well defined.\n",
    "If the key column in both the left and right array contains duplicates, then the result is a many-to-many merge.\n",
    "This will be perhaps most clear with a concrete example.\n",
    "Consider the following, where we have a ``DataFrame`` showing one or more skills associated with a particular group.\n",
    "By performing a many-to-many join, we can recover the skills associated with any individual person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d4eb7-c379-46da-a54b-d7c822e56043",
   "metadata": {
    "id": "HDHf9xLyxynO",
    "outputId": "1b46ba2e-190b-4baa-f355-b85e56d755bc"
   },
   "outputs": [],
   "source": [
    "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
    "                              'Engineering', 'Engineering', 'HR', 'HR'],\n",
    "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
    "                               'spreadsheets', 'organization']})\n",
    "display('df1', 'df5', \"pd.merge(df1, df5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e3b5a-6ceb-48b7-b70d-2bfb4336fe50",
   "metadata": {
    "id": "oEyaFor5xynQ"
   },
   "source": [
    "These three types of joins can be used with other Pandas tools to implement a wide array of functionality.\n",
    "But in practice, datasets are rarely as clean as the one we're working with here.\n",
    "In the following section we'll consider some of the options provided by ``pd.merge()`` that enable you to tune how the join operations work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2589cd62-1440-42ec-819a-d0aec339d3dd",
   "metadata": {
    "id": "FxXdu9kqxynR"
   },
   "source": [
    "## Specification of the Merge Key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3f55f-5597-4b19-a7e7-4181254dd8b5",
   "metadata": {
    "id": "36DbSr6vxynS"
   },
   "source": [
    "We've already seen the default behavior of ``pd.merge()``: it looks for one or more matching column names between the two inputs, and uses this as the key.\n",
    "However, often the column names will not match so nicely, and ``pd.merge()`` provides a variety of options for handling this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335859a1-9e1f-426c-b0cd-22903869ee20",
   "metadata": {
    "id": "Pi9sbFF1xynU"
   },
   "source": [
    "### The ``on`` keyword\n",
    "\n",
    "Most simply, you can explicitly specify the name of the key column using the ``on`` keyword, which takes a column name or a list of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edcd85e-df03-42e3-8e19-a238ada3a34c",
   "metadata": {
    "id": "Bn6JQSw_xynV",
    "outputId": "3aa5cd78-92b8-401e-c38b-a931f5ec37d7"
   },
   "outputs": [],
   "source": [
    "display('df1', 'df2', \"pd.merge(df1, df2, on='employee')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504cc7a-f31a-4ada-a6f5-623f42be141f",
   "metadata": {
    "id": "0HrUCHvExynW"
   },
   "source": [
    "This option works only if both the left and right ``DataFrame``s have the specified column name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881866eb-e15c-46a4-bb24-e2d50f023e99",
   "metadata": {
    "id": "O6mlYLPgxynW"
   },
   "source": [
    "### The ``left_on`` and ``right_on`` keywords\n",
    "\n",
    "At times you may wish to merge two datasets with different column names; for example, we may have a dataset in which the employee name is labeled as \"name\" rather than \"employee\".\n",
    "In this case, we can use the ``left_on`` and ``right_on`` keywords to specify the two column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b075029-4ee6-48d7-a91b-cdb666e77eb0",
   "metadata": {
    "id": "l1oXxZsJxynX",
    "outputId": "3e627bb1-ace1-4cd9-c872-0e6947f8290d"
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'name': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'salary': [70000, 80000, 120000, 90000]})\n",
    "display('df1', 'df3', 'pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\")')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4557a631-8aa2-4eac-9f39-997fdd992c06",
   "metadata": {
    "id": "Ecea_ydUxynX"
   },
   "source": [
    "The result has a redundant column that we can drop if desired–for example, by using the ``drop()`` method of ``DataFrame``s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3f0086-6312-48ae-87dd-2071750915a9",
   "metadata": {
    "id": "L_cPAxy7xynY",
    "outputId": "6677a12a-2384-4876-fb1b-fc7a50a7e18b"
   },
   "outputs": [],
   "source": [
    "pd.merge(df1, df3, left_on=\"employee\", right_on=\"name\").drop('name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579072b5-ae50-4b01-a8a2-5900fefce76e",
   "metadata": {
    "id": "NAeTNGYdxynZ"
   },
   "source": [
    "### The ``left_index`` and ``right_index`` keywords\n",
    "\n",
    "Sometimes, rather than merging on a column, you would instead like to merge on an index.\n",
    "For example, your data might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664d4d13-bfd0-44b2-a4a8-d26bbd8159ee",
   "metadata": {
    "id": "R0YiC1Ckxyna",
    "outputId": "6fc61a95-eac5-4776-bac3-27a3ed864466"
   },
   "outputs": [],
   "source": [
    "df1a = df1.set_index('employee')\n",
    "df2a = df2.set_index('employee')\n",
    "display('df1a', 'df2a')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c551fe-c489-4382-853e-86be28c161ce",
   "metadata": {
    "id": "vm1zoHcSxynb"
   },
   "source": [
    "You can use the index as the key for merging by specifying the ``left_index`` and/or ``right_index`` flags in ``pd.merge()``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5201d7e-0e71-4172-b77e-e3803331a42f",
   "metadata": {
    "id": "y3jtuHIZxynb",
    "outputId": "c4a5d718-1997-4208-95b2-4bd5e724847d"
   },
   "outputs": [],
   "source": [
    "display('df1a', 'df2a',\n",
    "        \"pd.merge(df1a, df2a, left_index=True, right_index=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c44e3b-3a47-4e6e-9b1a-93f6ec9ce364",
   "metadata": {
    "id": "hk2emVjhxync"
   },
   "source": [
    "For convenience, ``DataFrame``s implement the ``join()`` method, which performs a merge that defaults to joining on indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed26587-a46e-409e-a220-593e105bcf16",
   "metadata": {
    "id": "blm28FA3xynd",
    "outputId": "43e029a3-bf68-4d4c-dfa0-0875af0f0af8"
   },
   "outputs": [],
   "source": [
    "display('df1a', 'df2a', 'df1a.join(df2a)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5701909-88a6-403e-b42c-38a340805232",
   "metadata": {
    "id": "8FcSf-o2xynd"
   },
   "source": [
    "If you'd like to mix indices and columns, you can combine ``left_index`` with ``right_on`` or ``left_on`` with ``right_index`` to get the desired behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e545a8-0222-48b2-ad75-8dd4e9cfa76e",
   "metadata": {
    "id": "AsKpP_0Txyne",
    "outputId": "67b20adf-8fa6-4f3d-e292-1b2421e2eb15"
   },
   "outputs": [],
   "source": [
    "display('df1a', 'df3', \"pd.merge(df1a, df3, left_index=True, right_on='name')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d4a6c-4da0-47c0-9a76-f226f21018b0",
   "metadata": {
    "id": "wsqjCmxzxynf"
   },
   "source": [
    "All of these options also work with multiple indices and/or multiple columns; the interface for this behavior is very intuitive.\n",
    "For more information on this, see the [\"Merge, Join, and Concatenate\" section](http://pandas.pydata.org/pandas-docs/stable/merging.html) of the Pandas documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c1539-244c-421a-bb23-16bf0e8a4a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df5b77f-bdd5-4322-a865-f005b17a308e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78195bfb-b8e0-4302-9349-8423a48cf22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b59bb0e6-3e12-43ba-8a7c-599a8278ae0b",
   "metadata": {
    "id": "8_otjNO-x_-2"
   },
   "source": [
    "# Aggregation and Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0808e1c7-a638-474e-920d-290d28ded197",
   "metadata": {
    "id": "x-jj6i4Rx__G"
   },
   "source": [
    "## GroupBy: Split, Apply, Combine\n",
    "\n",
    "Simple aggregations can give you a flavor of your dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the so-called ``groupby`` operation.\n",
    "The name \"group by\" comes from a command in the SQL database language, but it is perhaps more illuminative to think of it in the terms first coined by Hadley Wickham of Rstats fame: *split, apply, combine*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b63ac6-213e-4425-84cf-d6908ddab457",
   "metadata": {
    "id": "6XLfdqH6x__H"
   },
   "source": [
    "### Split, apply, combine\n",
    "\n",
    "A canonical example of this split-apply-combine operation, where the \"apply\" is a summation aggregation, is illustrated in this figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cf3709-94b2-4e4f-8c7b-ce72df665e1d",
   "metadata": {
    "id": "1d-Ym-y6x__H"
   },
   "source": [
    "![](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/03.08-split-apply-combine.png?raw=1)\n",
    "[figure source in Appendix](06.00-Figure-Code.ipynb#Split-Apply-Combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf5629d-3908-4361-bdbb-dad968d9adbf",
   "metadata": {
    "id": "o6p3O7Fgx__H"
   },
   "source": [
    "This makes clear what the ``groupby`` accomplishes:\n",
    "\n",
    "- The *split* step involves breaking up and grouping a ``DataFrame`` depending on the value of the specified key.\n",
    "- The *apply* step involves computing some function, usually an aggregate, transformation, or filtering, within the individual groups.\n",
    "- The *combine* step merges the results of these operations into an output array.\n",
    "\n",
    "While this could certainly be done manually using some combination of the masking, aggregation, and merging commands covered earlier, an important realization is that *the intermediate splits do not need to be explicitly instantiated*. Rather, the ``GroupBy`` can (often) do this in a single pass over the data, updating the sum, mean, count, min, or other aggregate for each group along the way.\n",
    "The power of the ``GroupBy`` is that it abstracts away these steps: the user need not think about *how* the computation is done under the hood, but rather thinks about the *operation as a whole*.\n",
    "\n",
    "As a concrete example, let's take a look at using Pandas for the computation shown in this diagram.\n",
    "We'll start by creating the input ``DataFrame``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84bf556-f7eb-420b-919a-d80bf0787a88",
   "metadata": {
    "id": "a3q_ORFqx__I",
    "outputId": "715fd02c-3926-456b-e85e-6d9d5502353b"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'data': range(6)}, columns=['key', 'data'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53909a82-ba4d-4399-9b81-0cb0cb95df0d",
   "metadata": {
    "id": "sVBoXRxux__J"
   },
   "source": [
    "The most basic split-apply-combine operation can be computed with the ``groupby()`` method of ``DataFrame``s, passing the name of the desired key column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e730df2-5d6c-4ded-bd96-ff7e5e5c2312",
   "metadata": {
    "id": "C65P4bCrx__J",
    "outputId": "ad9f2fd3-0221-488f-861c-67905323f49b"
   },
   "outputs": [],
   "source": [
    "df.groupby('key')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd127b-2d5d-4033-9ae3-de3e79108f29",
   "metadata": {
    "id": "QMz6tKFwx__K"
   },
   "source": [
    "Notice that what is returned is not a set of ``DataFrame``s, but a ``DataFrameGroupBy`` object.\n",
    "This object is where the magic is: you can think of it as a special view of the ``DataFrame``, which is poised to dig into the groups but does no actual computation until the aggregation is applied.\n",
    "This \"lazy evaluation\" approach means that common aggregates can be implemented very efficiently in a way that is almost transparent to the user.\n",
    "\n",
    "To produce a result, we can apply an aggregate to this ``DataFrameGroupBy`` object, which will perform the appropriate apply/combine steps to produce the desired result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361dc50a-b4c8-44ab-a781-31e922047d5b",
   "metadata": {
    "id": "-9hh5fmMx__K",
    "outputId": "0020c125-3965-4eb7-e2e0-67564e58c508"
   },
   "outputs": [],
   "source": [
    "df.groupby('key').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e9f6c8-750a-4c53-a22e-1911d85c5bb7",
   "metadata": {
    "id": "l7EfW0rNx__K"
   },
   "source": [
    "The ``sum()`` method is just one possibility here; you can apply virtually any common Pandas or NumPy aggregation function, as well as virtually any valid ``DataFrame`` operation, as we will see in the following discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb35842-2463-46c0-a72e-67cdd7096322",
   "metadata": {
    "id": "ZNbk3hvMx__K"
   },
   "source": [
    "### The GroupBy object\n",
    "\n",
    "The ``GroupBy`` object is a very flexible abstraction.\n",
    "In many ways, you can simply treat it as if it's a collection of ``DataFrame``s, and it does the difficult things under the hood. Let's see some examples using the Planets data.\n",
    "\n",
    "Perhaps the most important operations made available by a ``GroupBy`` are *aggregate*, *filter*, *transform*, and *apply*.\n",
    "We'll discuss each of these more fully in [\"Aggregate, Filter, Transform, Apply\"](#Aggregate,-Filter,-Transform,-Apply), but before that let's introduce some of the other functionality that can be used with the basic ``GroupBy`` operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d92a57-a248-42a2-9a67-a3ed0eacb163",
   "metadata": {
    "id": "NeKOis6Tx__L"
   },
   "source": [
    "#### Column indexing\n",
    "\n",
    "The ``GroupBy`` object supports column indexing in the same way as the ``DataFrame``, and returns a modified ``GroupBy`` object.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b14e24d-dc96-494f-8d83-5380ed4431e0",
   "metadata": {
    "id": "Gh56XckHx__L",
    "outputId": "2e5c300f-a95a-4c13-ca43-ddf859e10fa2"
   },
   "outputs": [],
   "source": [
    "planets.groupby('method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751f218-adb4-4f99-9496-f4a219502410",
   "metadata": {
    "id": "5v_ErUhSx__L",
    "outputId": "284ed6d5-1b83-4fe8-917d-2b943038a55a"
   },
   "outputs": [],
   "source": [
    "planets.groupby('method')['orbital_period']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cfb3d9-cee2-4ba1-9673-df1eee407729",
   "metadata": {
    "id": "Bo4wIpSYx__M"
   },
   "source": [
    "Here we've selected a particular ``Series`` group from the original ``DataFrame`` group by reference to its column name.\n",
    "As with the ``GroupBy`` object, no computation is done until we call some aggregate on the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d47177b-0ff7-47c7-8344-c112aaed9a69",
   "metadata": {
    "id": "7WkitVVSx__M",
    "outputId": "23f35c6d-003f-4cc7-aff0-2568b8a5e34b"
   },
   "outputs": [],
   "source": [
    "planets.groupby('method')['orbital_period'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685e57c5-fdde-493e-945f-2e7a66c39b94",
   "metadata": {
    "id": "NSGkW0Iax__M"
   },
   "source": [
    "This gives an idea of the general scale of orbital periods (in days) that each method is sensitive to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c58284-1bbe-4728-9b88-8bbde3c4348d",
   "metadata": {
    "id": "q0yTKw-bx__M"
   },
   "source": [
    "#### Iteration over groups\n",
    "\n",
    "The ``GroupBy`` object supports direct iteration over the groups, returning each group as a ``Series`` or ``DataFrame``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991fc9b-1913-4fd1-a0e6-c1af7256a1b8",
   "metadata": {
    "id": "btmJZSUFx__N",
    "outputId": "409b2819-6927-4569-ff62-8879538b9dc6"
   },
   "outputs": [],
   "source": [
    "for (method, group) in planets.groupby('method'):\n",
    "    print(\"{0:30s} shape={1}\".format(method, group.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3148f579-28f4-4ae0-98b2-fbd9ca8ad697",
   "metadata": {
    "id": "nEYrKrjHx__N"
   },
   "source": [
    "This can be useful for doing certain things manually, though it is often much faster to use the built-in ``apply`` functionality, which we will discuss momentarily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf736d-845d-4942-9433-6b98f189cc39",
   "metadata": {
    "id": "HMYGJ74sx__N"
   },
   "source": [
    "#### Dispatch methods\n",
    "\n",
    "Through some Python class magic, any method not explicitly implemented by the ``GroupBy`` object will be passed through and called on the groups, whether they are ``DataFrame`` or ``Series`` objects.\n",
    "For example, you can use the ``describe()`` method of ``DataFrame``s to perform a set of aggregations that describe each group in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f58381-53a4-4f7c-95c9-5c094b23c72d",
   "metadata": {
    "id": "Hq950o-gx__N",
    "outputId": "33c79fee-adbc-472c-8348-4b54901d78e2"
   },
   "outputs": [],
   "source": [
    "planets.groupby('method')['year'].describe().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be4ab4-5eae-4ac7-b959-2133ea080db2",
   "metadata": {
    "id": "FT-Rdlmpx__O"
   },
   "source": [
    "Looking at this table helps us to better understand the data: for example, the vast majority of planets have been discovered by the Radial Velocity and Transit methods, though the latter only became common (due to new, more accurate telescopes) in the last decade.\n",
    "The newest methods seem to be Transit Timing Variation and Orbital Brightness Modulation, which were not used to discover a new planet until 2011.\n",
    "\n",
    "This is just one example of the utility of dispatch methods.\n",
    "Notice that they are applied *to each individual group*, and the results are then combined within ``GroupBy`` and returned.\n",
    "Again, any valid ``DataFrame``/``Series`` method can be used on the corresponding ``GroupBy`` object, which allows for some very flexible and powerful operations!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9929b737-acd9-4c6a-8257-9f5a3adf4a94",
   "metadata": {
    "id": "cUYGYUMbx__O"
   },
   "source": [
    "### Aggregate, filter, transform, apply\n",
    "\n",
    "The preceding discussion focused on aggregation for the combine operation, but there are more options available.\n",
    "In particular, ``GroupBy`` objects have ``aggregate()``, ``filter()``, ``transform()``, and ``apply()`` methods that efficiently implement a variety of useful operations before combining the grouped data.\n",
    "\n",
    "For the purpose of the following subsections, we'll use this ``DataFrame``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19616f77-540c-4c5a-a16e-b1c138099e7f",
   "metadata": {
    "id": "mJnuPcbVx__O",
    "outputId": "c43ad8d4-4f12-459a-86c7-22d73c365e8b"
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'data1': range(6),\n",
    "                   'data2': rng.randint(0, 10, 6)},\n",
    "                   columns = ['key', 'data1', 'data2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f6c9e-6d93-42c8-ac33-c7b6484350a6",
   "metadata": {
    "id": "soh-6XPlx__P"
   },
   "source": [
    "#### Aggregation\n",
    "\n",
    "We're now familiar with ``GroupBy`` aggregations with ``sum()``, ``median()``, and the like, but the ``aggregate()`` method allows for even more flexibility.\n",
    "It can take a string, a function, or a list thereof, and compute all the aggregates at once.\n",
    "Here is a quick example combining all these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d179c5ba-c251-4c36-81e2-ca554a8ed975",
   "metadata": {
    "id": "7PaZg21tx__P",
    "outputId": "802e3111-a1ea-4fbe-d98d-457665acf850"
   },
   "outputs": [],
   "source": [
    "df.groupby('key').aggregate(['min', np.median, max])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af7257-afcc-4cb7-bca9-b9d1cb9db1a9",
   "metadata": {
    "id": "_4e6RLn0x__Q"
   },
   "source": [
    "Another useful pattern is to pass a dictionary mapping column names to operations to be applied on that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef81f6f-265e-4a25-b702-02c8133aaad7",
   "metadata": {
    "id": "VICotA9tx__Q",
    "outputId": "8f6107d0-f2ff-496a-8ab5-c9c7b362d7d6"
   },
   "outputs": [],
   "source": [
    "df.groupby('key').aggregate({'data1': 'min',\n",
    "                             'data2': 'max'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144c554b-8d0e-4108-b9a9-b17e1c9f89ef",
   "metadata": {
    "id": "5GkpgGx-x__Q"
   },
   "source": [
    "#### Filtering\n",
    "\n",
    "A filtering operation allows you to drop data based on the group properties.\n",
    "For example, we might want to keep all groups in which the standard deviation is larger than some critical value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72390f3a-a7aa-47d7-b301-96559153b4cf",
   "metadata": {
    "id": "pxF6ZZPtx__Q",
    "outputId": "9de2f291-7d63-4751-f5e7-f98dd175943e"
   },
   "outputs": [],
   "source": [
    "def filter_func(x):\n",
    "    return x['data2'].std() > 4\n",
    "\n",
    "display('df', \"df.groupby('key').std()\", \"df.groupby('key').filter(filter_func)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd202be1-71ba-41b2-bfce-1ff1fe3c3b16",
   "metadata": {
    "id": "cS5VycFDx__R"
   },
   "source": [
    "The filter function should return a Boolean value specifying whether the group passes the filtering. Here because group A does not have a standard deviation greater than 4, it is dropped from the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3df2fe-1869-42df-ba52-881123100f0c",
   "metadata": {
    "id": "j12rxihKx__R"
   },
   "source": [
    "#### Transformation\n",
    "\n",
    "While aggregation must return a reduced version of the data, transformation can return some transformed version of the full data to recombine.\n",
    "For such a transformation, the output is the same shape as the input.\n",
    "A common example is to center the data by subtracting the group-wise mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b6b06c-3338-40e4-a310-4a77c255de77",
   "metadata": {
    "id": "9TpUdlVex__R",
    "outputId": "847f5878-deb8-4ebd-a2aa-1b2c3e2e6426"
   },
   "outputs": [],
   "source": [
    "df.groupby('key').transform(lambda x: x - x.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5014dd-f3ec-4d4c-98bb-6fd53551af32",
   "metadata": {
    "id": "DPFGQ4Osx__S"
   },
   "source": [
    "#### The apply() method\n",
    "\n",
    "The ``apply()`` method lets you apply an arbitrary function to the group results.\n",
    "The function should take a ``DataFrame``, and return either a Pandas object (e.g., ``DataFrame``, ``Series``) or a scalar; the combine operation will be tailored to the type of output returned.\n",
    "\n",
    "For example, here is an ``apply()`` that normalizes the first column by the sum of the second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2db9ca-dcd6-418a-b1a4-d88cd643d61b",
   "metadata": {
    "id": "NCaPXCcyx__S",
    "outputId": "8851a437-20d3-4abc-9496-1188819bf8ac"
   },
   "outputs": [],
   "source": [
    "def norm_by_data2(x):\n",
    "    # x is a DataFrame of group values\n",
    "    x['data1'] /= x['data2'].sum()\n",
    "    return x\n",
    "\n",
    "display('df', \"df.groupby('key').apply(norm_by_data2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a950f7-017b-4c0b-b870-3edd8a9d4193",
   "metadata": {
    "id": "qhcCiLVJx__S"
   },
   "source": [
    "``apply()`` within a ``GroupBy`` is quite flexible: the only criterion is that the function takes a ``DataFrame`` and returns a Pandas object or scalar; what you do in the middle is up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c003544-6d19-4b55-9e9c-f9eb57b5b2e9",
   "metadata": {
    "id": "OGypI89Lx__S"
   },
   "source": [
    "### Specifying the split key\n",
    "\n",
    "In the simple examples presented before, we split the ``DataFrame`` on a single column name.\n",
    "This is just one of many options by which the groups can be defined, and we'll go through some other options for group specification here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05054f68-e50e-4afe-8b73-3787498e6e97",
   "metadata": {
    "id": "8CRhFI5Qx__S"
   },
   "source": [
    "#### A list, array, series, or index providing the grouping keys\n",
    "\n",
    "The key can be any series or list with a length matching that of the ``DataFrame``. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809e57e-1a73-4545-b6b7-9cddb10f4a69",
   "metadata": {
    "id": "uXoSO1JBx__T",
    "outputId": "8031ef5b-f33f-43f7-91c8-42700509da63"
   },
   "outputs": [],
   "source": [
    "L = [0, 1, 0, 1, 2, 0]\n",
    "display('df', 'df.groupby(L).sum()')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b876e6f0-a5b5-46b0-8f4d-ccb805dc8aaa",
   "metadata": {
    "id": "ULOH5nqGx__T"
   },
   "source": [
    "Of course, this means there's another, more verbose way of accomplishing the ``df.groupby('key')`` from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536bb3f-d5d6-4390-b5ae-4649d5552790",
   "metadata": {
    "id": "DRe_sjYZx__T",
    "outputId": "0a497228-7bff-4836-b9df-10bdde8bd20b"
   },
   "outputs": [],
   "source": [
    "display('df', \"df.groupby(df['key']).sum()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a06cea-1f53-4222-bfdd-1071bfdd5c5a",
   "metadata": {
    "id": "lk-QQ9gZx__T"
   },
   "source": [
    "#### A dictionary or series mapping index to group\n",
    "\n",
    "Another method is to provide a dictionary that maps index values to the group keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9191084-65c5-409f-abb8-178acb3f7b95",
   "metadata": {
    "id": "UZVb8AOPx__T",
    "outputId": "e5eb0a29-e8c6-45d8-bb1a-15fddc50c7d1"
   },
   "outputs": [],
   "source": [
    "df2 = df.set_index('key')\n",
    "mapping = {'A': 'vowel', 'B': 'consonant', 'C': 'consonant'}\n",
    "display('df2', 'df2.groupby(mapping).sum()')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc76a83-a488-4e02-9b88-0080b384c239",
   "metadata": {
    "id": "XzpzuXkPx__U"
   },
   "source": [
    "#### Any Python function\n",
    "\n",
    "Similar to mapping, you can pass any Python function that will input the index value and output the group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf4a7b-31b7-4402-9619-9bf16c4744c0",
   "metadata": {
    "id": "Ugj2yeyKx__U",
    "outputId": "f0d09d6b-a55f-4bd1-c496-fd9df7e68ded"
   },
   "outputs": [],
   "source": [
    "display('df2', 'df2.groupby(str.lower).mean()')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a57457e-b3e1-44cc-88ee-99347e9d735b",
   "metadata": {
    "id": "YbKEMqzZx__U"
   },
   "source": [
    "#### A list of valid keys\n",
    "\n",
    "Further, any of the preceding key choices can be combined to group on a multi-index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2fc0c4-cf83-4c66-bbd2-e5de57ccb367",
   "metadata": {
    "id": "MydHBkrSx__U",
    "outputId": "b3bd79e2-606a-4779-995e-b9a7c3bec06f"
   },
   "outputs": [],
   "source": [
    "df2.groupby([str.lower, mapping]).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
